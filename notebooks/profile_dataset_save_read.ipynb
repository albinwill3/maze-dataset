{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling of `maze_dataset` serializing/loading/saving/reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Callable, Any\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from muutils.statcounter import StatCounter\n",
    "from muutils.timeit_fancy import timeit_fancy, FancyTimeitResult\n",
    "\n",
    "from maze_dataset import (\n",
    "    MazeDataset,\n",
    "    MazeDatasetConfig,\n",
    "    set_serialize_minimal_threshold,\n",
    ")\n",
    "from maze_dataset.generation.generators import GENERATORS_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs: list[MazeDatasetConfig] = [\n",
    "    MazeDatasetConfig(\n",
    "        name=\"test\",\n",
    "        grid_n=grid_n,\n",
    "        n_mazes=n_mazes,\n",
    "        maze_ctor=GENERATORS_MAP[\"gen_dfs\"],\n",
    "    )\n",
    "    for grid_n, n_mazes in itertools.product(\n",
    "        [10],\n",
    "        np.logspace(1, 2, 2, dtype=int).tolist(),  # 100, for CI tests\n",
    "        # np.logspace(1, 3, 5, dtype=int).tolist(), # 1k\n",
    "        # np.logspace(0, 4, 9, dtype=int).tolist(), # 10k, notebook results from this set\n",
    "    )\n",
    "]\n",
    "\n",
    "datasets: list[MazeDataset] = [\n",
    "    MazeDataset.from_config(cfg, load_local=False) for cfg in cfgs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns: list[str] = [\n",
    "    \"grid_n\",\n",
    "    \"n_mazes\",\n",
    "    \"serialize\",\n",
    "    \"serialize_minimal\",\n",
    "    \"load\",\n",
    "    \"load_minimal\",\n",
    "    \"save\",\n",
    "    \"save_minimal\",\n",
    "    \"read\",\n",
    "    \"read_minimal\",\n",
    "]\n",
    "speeds_data: list[dict] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped_timeit_fancy(\n",
    "    name: str,\n",
    "    function: Callable,\n",
    "    do_profiling: bool,\n",
    "    repeats: int,\n",
    "    timing_stat: Callable[[StatCounter], float],\n",
    ") -> tuple[dict, Any]:\n",
    "    output: dict = dict()\n",
    "\n",
    "    result: FancyTimeitResult = timeit_fancy(\n",
    "        function,\n",
    "        get_return=True,\n",
    "        do_profiling=do_profiling,\n",
    "        repeats=repeats,\n",
    "    )\n",
    "\n",
    "    output[name] = timing_stat(result.timings)\n",
    "    output[f\"{name}:stats\"] = result.timings\n",
    "    if do_profiling:\n",
    "        output[f\"{name}:profiling\"] = result.profile\n",
    "\n",
    "    return output, result.return_value\n",
    "\n",
    "\n",
    "def measure_dataset_speed(\n",
    "    d: MazeDataset,\n",
    "    do_profiling: bool = True,\n",
    "    repeats: int = 1,\n",
    "    timing_stat: Callable[[StatCounter], float] = StatCounter.min,\n",
    ") -> dict:\n",
    "    if repeats > 1:\n",
    "        warnings.warn(\n",
    "            \"Repeats > 1, results might not be accurate due to generation metadata being collected.\"\n",
    "        )\n",
    "    kwargs_fancy_timeit: dict = dict(\n",
    "        do_profiling=do_profiling,\n",
    "        timing_stat=timing_stat,\n",
    "        repeats=repeats,\n",
    "    )\n",
    "    set_serialize_minimal_threshold(None)\n",
    "    _d_cpy: MazeDataset = copy.deepcopy(d)\n",
    "    # set up row data\n",
    "    row_data: dict = dict(\n",
    "        grid_n=d.cfg.grid_n,\n",
    "        n_mazes=d.cfg.n_mazes,\n",
    "    )\n",
    "    # serialization & loading\n",
    "    info_serialize, result_serialize = wrapped_timeit_fancy(\n",
    "        \"serialize_full\", _d_cpy._serialize_full, **kwargs_fancy_timeit\n",
    "    )\n",
    "    row_data.update(info_serialize)\n",
    "    _d_cpy = copy.deepcopy(d)\n",
    "\n",
    "    info_serialize_min, result_serialize_min = wrapped_timeit_fancy(\n",
    "        \"serialize_minimal\", _d_cpy._serialize_minimal, **kwargs_fancy_timeit\n",
    "    )\n",
    "    row_data.update(info_serialize_min)\n",
    "    _d_cpy = copy.deepcopy(d)\n",
    "\n",
    "    # info_serialize_min_alt, result_serialize_min_alt = wrapped_timeit_fancy(\n",
    "    #     'serialize_minimal_alt', _d_cpy._serialize_minimal_alt, **kwargs_fancy_timeit\n",
    "    # )\n",
    "    # row_data.update(info_serialize_min_alt)\n",
    "    _d_cpy = copy.deepcopy(d)\n",
    "    info_serialize_cat, result_serialize_cat = wrapped_timeit_fancy(\n",
    "        \"serialize_minimal_soln_cat\",\n",
    "        _d_cpy._serialize_minimal_soln_cat,\n",
    "        **kwargs_fancy_timeit,\n",
    "    )\n",
    "    row_data.update(info_serialize_cat)\n",
    "    _d_cpy = copy.deepcopy(d)\n",
    "\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"load_legacy\",\n",
    "            lambda: MazeDataset._load_legacy(result_serialize),\n",
    "            **kwargs_fancy_timeit,\n",
    "        )[0]\n",
    "    )\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"load_full\",\n",
    "            lambda: MazeDataset._load_full(result_serialize),\n",
    "            **kwargs_fancy_timeit,\n",
    "        )[0]\n",
    "    )\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"load_minimal\",\n",
    "            lambda: MazeDataset._load_minimal(result_serialize_min),\n",
    "            **kwargs_fancy_timeit,\n",
    "        )[0]\n",
    "    )\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"load_minimal_soln_cat\",\n",
    "            lambda: MazeDataset._load_minimal_soln_cat(result_serialize_cat),\n",
    "            **kwargs_fancy_timeit,\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"load_full\",\n",
    "            lambda: MazeDataset._load_full(result_serialize),\n",
    "            **kwargs_fancy_timeit,\n",
    "        )[0]\n",
    "    )\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"load_minimal\",\n",
    "            lambda: MazeDataset._load_minimal(result_serialize_min),\n",
    "            **kwargs_fancy_timeit,\n",
    "        )[0]\n",
    "    )\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"load_minimal_soln_cat\",\n",
    "            lambda: MazeDataset._load_minimal_soln_cat(result_serialize_cat),\n",
    "            **kwargs_fancy_timeit,\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "    # saving and loading\n",
    "    path_default: str = f\"../data/{d.cfg.to_fname()}.zanj\"\n",
    "    path_min: str = f\"../data/{d.cfg.to_fname()}_min.zanj\"\n",
    "\n",
    "    # default\n",
    "    set_serialize_minimal_threshold(None)\n",
    "    _d_cpy = copy.deepcopy(d)\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"save\", lambda: _d_cpy.save(file_path=path_default), **kwargs_fancy_timeit\n",
    "        )[0]\n",
    "    )\n",
    "    _d_cpy = copy.deepcopy(d)\n",
    "\n",
    "    # read_legacy\n",
    "    set_serialize_minimal_threshold(-1)\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"read_legacy\",\n",
    "            lambda: MazeDataset.read(file_path=path_default),\n",
    "            **kwargs_fancy_timeit,\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "    # default read\n",
    "    set_serialize_minimal_threshold(None)\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"read\",\n",
    "            lambda: MazeDataset.read(file_path=path_default),\n",
    "            **kwargs_fancy_timeit,\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "    # minimal\n",
    "    set_serialize_minimal_threshold(0)\n",
    "    _d_cpy = copy.deepcopy(d)\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"save_minimal\",\n",
    "            lambda: _d_cpy.save(file_path=path_min),\n",
    "            **kwargs_fancy_timeit,\n",
    "        )[0]\n",
    "    )\n",
    "    _d_cpy = copy.deepcopy(d)\n",
    "\n",
    "    row_data.update(\n",
    "        wrapped_timeit_fancy(\n",
    "            \"read_minimal\",\n",
    "            lambda: MazeDataset.read(file_path=path_min),\n",
    "            **kwargs_fancy_timeit,\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "    # asserts\n",
    "    # assert d == read_default\n",
    "    # assert d == read_minimal\n",
    "\n",
    "    # reset cfg?\n",
    "    set_serialize_minimal_threshold(None)\n",
    "\n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(datasets):\n",
    "    print(f\"Profiling {i + 1}/{len(datasets)}:\\t{d.cfg}\")\n",
    "    result = measure_dataset_speed(d)\n",
    "    speeds_data.append(result)\n",
    "    cols_short: str = str({k: v for k, v in result.items() if \":\" not in k})\n",
    "    print(f\"\\t{cols_short}\")\n",
    "    print(f\"\\t{str(d.cfg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEEDS: pd.DataFrame = pd.DataFrame(speeds_data)\n",
    "\n",
    "SPEEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_speedups(speeds: pd.DataFrame) -> pd.DataFrame:\n",
    "    # for prefix in column_measurement_prefixes:\n",
    "    #     speeds[f'{prefix}_speedup'] = speeds[f'{prefix}_full'] / speeds[f'{prefix}_minimal']\n",
    "    speeds[\"serialize/speedup\"] = speeds[\"serialize_full\"] / speeds[\"serialize_minimal\"]\n",
    "    speeds[\"load/speedup\"] = speeds[\"load_full\"] / speeds[\"load_minimal\"]\n",
    "    speeds[\"save/speedup\"] = speeds[\"save\"] / speeds[\"save_minimal\"]\n",
    "    speeds[\"read/speedup\"] = speeds[\"read\"] / speeds[\"read_minimal\"]\n",
    "\n",
    "    return speeds\n",
    "\n",
    "\n",
    "SPEEDS = compute_speedups(SPEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEEDS: pd.DataFrame = pd.DataFrame(speeds_data)\n",
    "\n",
    "# SPEEDS.loc[:,\"load_legacy\":\"load_minimal_soln_cat:profiling\"]\n",
    "SPEEDS.loc[:, \"read_legacy\":\"read:profiling\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEEDS.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_speedups(speeds: pd.DataFrame) -> pd.DataFrame:\n",
    "    # for prefix in column_measurement_prefixes:\n",
    "    #     speeds[f'{prefix}_speedup'] = speeds[f'{prefix}_full'] / speeds[f'{prefix}_minimal']\n",
    "    speeds[\"serialize/speedup\"] = speeds[\"serialize_full\"] / speeds[\"serialize_minimal\"]\n",
    "    speeds[\"load_minimal/speedup\"] = speeds[\"load_legacy\"] / speeds[\"load_minimal\"]\n",
    "    speeds[\"load/speedup\"] = speeds[\"load_legacy\"] / speeds[\"load_full\"]\n",
    "    speeds[\"save/speedup\"] = speeds[\"save\"] / speeds[\"save_minimal\"]\n",
    "    speeds[\"read_minimal/speedup\"] = speeds[\"read_legacy\"] / speeds[\"read_minimal\"]\n",
    "    speeds[\"read/speedup\"] = speeds[\"read_legacy\"] / speeds[\"read\"]\n",
    "\n",
    "    return speeds\n",
    "\n",
    "\n",
    "SPEEDS = compute_speedups(SPEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEEDS[[c for c in SPEEDS.columns if \":\" not in c]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_speeds(\n",
    "    speeds: pd.DataFrame,\n",
    "    column_measurement_prefixes: list[str] = [\"serialize\", \"load\", \"save\", \"read\"],\n",
    ") -> None:\n",
    "    n_measurements: int = len(column_measurement_prefixes)\n",
    "    fig, axs = plt.subplots(2, n_measurements, figsize=(n_measurements * 5, 10))\n",
    "\n",
    "    unique_grid_ns: list[int] = speeds[\"grid_n\"].unique().tolist()\n",
    "\n",
    "    for i, prefix in enumerate(column_measurement_prefixes):\n",
    "        print(f\"Plotting {prefix} timings and speedups\")\n",
    "        for grid_n in unique_grid_ns:\n",
    "            print(f\"Plotting grid_n={grid_n}\")\n",
    "            # raw timings\n",
    "            ax_timings = axs[0, i]\n",
    "            speeds_masked = speeds[speeds[\"grid_n\"] == grid_n].sort_values(\"n_mazes\")\n",
    "            x_n_mazes = speeds_masked[\"n_mazes\"]\n",
    "\n",
    "            # Plotting\n",
    "            for col in speeds_masked.columns:\n",
    "                if (prefix in col) and (\"speedup\" not in col) and (\":\" not in col):\n",
    "                    ax_timings.plot(\n",
    "                        x_n_mazes,\n",
    "                        speeds_masked[col],\n",
    "                        \"x-\",\n",
    "                        label=f\"grid_n={grid_n}, {col}\",\n",
    "                    )\n",
    "\n",
    "            # Setting multiple properties with `set`\n",
    "            ax_timings.set(\n",
    "                xscale=\"log\",\n",
    "                yscale=\"log\",\n",
    "                xlabel=\"Number of mazes\",\n",
    "                ylabel=\"Runtime [sec]\",\n",
    "                title=f\"{prefix} timings\",\n",
    "            )\n",
    "            ax_timings.legend()\n",
    "\n",
    "            # speedups\n",
    "            ax_speedups = axs[1, i]\n",
    "            col_name: str = (\n",
    "                f\"{prefix}\" if prefix in (\"serialize\", \"save\") else f\"{prefix}_minimal\"\n",
    "            )\n",
    "            ax_speedups.plot(\n",
    "                x_n_mazes,\n",
    "                speeds_masked[f\"{col_name}/speedup\"],\n",
    "                \"x-\",\n",
    "                label=f\"grid_n={grid_n}\",\n",
    "            )\n",
    "\n",
    "            # Setting multiple properties with `set` for ax_speedups\n",
    "            ax_speedups.set(\n",
    "                xscale=\"log\",\n",
    "                yscale=\"log\",\n",
    "                xlabel=\"Number of mazes\",\n",
    "                ylabel=\"Speedup\",\n",
    "                title=f\"{col_name} speedups\",\n",
    "            )\n",
    "            ax_speedups.plot(\n",
    "                x_n_mazes,\n",
    "                speeds_masked[f\"{prefix}/speedup\"],\n",
    "                \"x-\",\n",
    "                label=f\"grid_n={grid_n}\",\n",
    "            )\n",
    "\n",
    "            # Setting multiple properties with `set` for ax_speedups\n",
    "            ax_speedups.set(\n",
    "                xscale=\"log\",\n",
    "                yscale=\"log\",\n",
    "                xlabel=\"Number of mazes\",\n",
    "                ylabel=\"Speedup\",\n",
    "                title=f\"{prefix} speedups\",\n",
    "            )\n",
    "            ax_speedups.legend()\n",
    "\n",
    "\n",
    "plot_speeds(SPEEDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speedups plotted on the bottom set of axes all show the `_minimal` compared to the legacy performance. `serialize_full` and `save` are unchanged from the legacy version, so speedups are plotted relative to those vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEEDS[[\"grid_n\", \"n_mazes\", \"serialize_minimal:profiling\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEEDS[\"load_minimal:profiling\"][len(SPEEDS) - 1].sort_stats(\"tottime\").print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
