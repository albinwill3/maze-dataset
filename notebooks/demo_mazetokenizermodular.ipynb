{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from muutils.misc import shorten_numerical_to_str\n",
    "from tqdm import tqdm\n",
    "\n",
    "from maze_dataset import (\n",
    "\tVOCAB,\n",
    "\tVOCAB_LIST,\n",
    "\tVOCAB_TOKEN_TO_INDEX,\n",
    "\tLatticeMazeGenerators,\n",
    "\tMazeDataset,\n",
    "\tMazeDatasetConfig,\n",
    "\tSolvedMaze,\n",
    ")\n",
    "from maze_dataset.plotting import MazePlot\n",
    "from maze_dataset.tokenization import (\n",
    "\tAdjListTokenizers,\n",
    "\tCoordTokenizers,\n",
    "\tEdgePermuters,\n",
    "\tEdgeSubsets,\n",
    "\tMazeTokenizer,\n",
    "\tMazeTokenizerModular,\n",
    "\tPathTokenizers,\n",
    "\tPromptSequencers,\n",
    "\tStepSizes,\n",
    "\tStepTokenizers,\n",
    "\tTargetTokenizers,\n",
    "\tTokenizationMode,\n",
    "\t_TokenizerElement,\n",
    ")\n",
    "from maze_dataset.tokenization.all_tokenizers import (\n",
    "\tMAZE_TOKENIZER_MODULAR_DEFAULT_VALIDATION_FUNCS,\n",
    "\tget_all_tokenizers,\n",
    ")\n",
    "from maze_dataset.tokenization.maze_tokenizer import get_all_tokenizer_hashes\n",
    "from maze_dataset.utils import all_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `MazeTokenizerModular` Initialization and Structure\n",
    "\n",
    "Initialiation can be done vai the default constructor or via `MazeTokenizerModular.from_legacy`. The latter is useful for converting a legacy `MazeTokenizer` into its equivalent `MazeTokenizerModular`.\n",
    "\n",
    "Most of the API for these tokenizers is contained in the `MazeTokenizerModular` class. The only time when users need to interact with the internal components of a `MazeTokenizerModular` is when initializing a non-default tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_default: MazeTokenizerModular = MazeTokenizerModular()\n",
    "mt_ctt: MazeTokenizerModular = MazeTokenizerModular.from_legacy(\n",
    "\tTokenizationMode.AOTP_CTT_indexed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects composing `MazeTokenizerModular` are all instances of `_TokenizerElement`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'maze_dataset.tokenization.maze_tokenizer.CoordTokenizers._CoordTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.EdgeGroupings._EdgeGrouping'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.EdgePermuters._EdgePermuter'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.EdgeSubsets._EdgeSubset'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.AdjListTokenizers._AdjListTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.TargetTokenizers._TargetTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.StepSizes._StepSize'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers._StepTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.PathTokenizers._PathTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers._PromptSequencer'>\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([str(elem) for elem in _TokenizerElement.__subclasses__()]))\n",
    "assert all(\n",
    "\tissubclass(elem, _TokenizerElement) for elem in _TokenizerElement.__subclasses__()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within a tokenizer, these `_TokenizerElement`s are structured in a nested dataclass tree. The tree is slightly different depending on the particular options selected. Below are shown 3 different tree representations of `mt_default`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AOTP `_TokenizerElement` Structure:\n",
      "\n",
      "MazeTokenizerModular\n",
      "\t_PromptSequencer\n",
      "\t\t_CoordTokenizer\n",
      "\t\t_AdjListTokenizer\n",
      "\t\t\t_EdgeGrouping\n",
      "\t\t\t_EdgeSubset\n",
      "\t\t\t_EdgePermuter\n",
      "\t\t_TargetTokenizer\n",
      "\t\t_PathTokenizer\n",
      "\t\t\t_StepSize\n",
      "\t\t\t_StepTokenizer\n",
      "\n",
      "Default tokenizer elements:\n",
      "\n",
      "MazeTokenizerModular\n",
      "\tAOTP\n",
      "\t\tUT\n",
      "\t\tAdjListCoord\n",
      "\t\t\tUngrouped\n",
      "\t\t\tConnectionEdges\n",
      "\t\t\tRandomCoords\n",
      "\t\tUnlabeled\n",
      "\t\tStepSequence\n",
      "\t\t\tSingles\n",
      "\t\t\tCoord\n",
      "\n",
      "\n",
      "Default tokenizer `name`:\n",
      "\n",
      "MazeTokenizerModular-AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))\n",
      "`MazeTokenizerModular` structure with all fields:\n",
      "\n",
      "MazeTokenizerModular:\n",
      "  AOTP:\n",
      "    adj_list_tokenizer:\n",
      "      AdjListCoord:\n",
      "        edge_grouping:\n",
      "          Ungrouped:\n",
      "            connection_token_ordinal: 1\n",
      "        edge_permuter:\n",
      "          RandomCoords: {}\n",
      "        edge_subset:\n",
      "          ConnectionEdges:\n",
      "            walls: false\n",
      "        post: true\n",
      "        pre: false\n",
      "        shuffle_d0: true\n",
      "    coord_tokenizer:\n",
      "      UT: {}\n",
      "    path_tokenizer:\n",
      "      StepSequence:\n",
      "        intra: false\n",
      "        post: false\n",
      "        pre: false\n",
      "        step_size:\n",
      "          Singles: {}\n",
      "        step_tokenizers:\n",
      "        - Coord: {}\n",
      "    target_tokenizer:\n",
      "      Unlabeled:\n",
      "        post: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAOTP `_TokenizerElement` Structure:\\n\")\n",
    "print(mt_default.tokenizer_element_tree(abstract=True))\n",
    "print(\"Default tokenizer elements:\\n\")\n",
    "print(mt_default.tokenizer_element_tree())\n",
    "print(\"\\nDefault tokenizer `name`:\\n\")\n",
    "print(mt_default.name)\n",
    "print(\"`MazeTokenizerModular` structure with all fields:\\n\")\n",
    "print(yaml.dump(mt_default.tokenizer_element_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are currently no other constructor methods. To construct a `MazeTokenizerModular` with other `TokenizerElement`s besides those available via `from_legacy`, the standard constructor with all parent `TokenizerElement`s in the tree must be used. Some `TokenizerElement`s also contain their own initialization arguments, most of which are `boolean`-typed. The most common arguments across all `TokenizerElement`s are named `pre`, `intra`, and `post`, which all control the option to add delimiter tokens to that part of the output. Other args are more specialized; see the class docstrings for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "\n",
    "All instances of `MazeTokenizerModular` uses a static vocabulary `VOCAB`, which is one of the main functional differences from `MazeTokenizer`. Direct access to the static vocabulary can be made through 3 constants:\n",
    "- `VOCAB`\n",
    "  - Extension of the `SPECIAL_TOKENS` dataclass\n",
    "  - Supports direct property attribution\n",
    "- `VOCAB_LIST: list[str]`\n",
    "  - Contains the vocabulary in a list\n",
    "  - Index of a token is its unique ID\n",
    "- `VOCAB_TOKEN_TO_INDEX: dict[str, int]`\n",
    "  - Inverse mapping of `VOCAB_LIST`, maps tokens to unique IDs\n",
    "\n",
    "The following shows a visualization of the first 5 elements of each constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"`VOCAB`: IsDataclass\")\n",
    "for i, t in enumerate(VOCAB):\n",
    "\tif i >= 5:\n",
    "\t\tbreak\n",
    "\tprint(f\"\\tVOCAB.{t} =\\t'{getattr(VOCAB, t)}'\")\n",
    "print(\"\\t...\")\n",
    "\n",
    "print(\"\\n`VOCAB_LIST`: list[str]\")\n",
    "for t in VOCAB_LIST[:5]:\n",
    "\tprint(f\"\\t'{t}'\")\n",
    "print(\"\\t...\")\n",
    "\n",
    "print(\"\\n`VOCAB_TOKEN_TO_INDEX`: dict[str, int]\")\n",
    "for t in VOCAB_TOKEN_TO_INDEX:\n",
    "\tif VOCAB_TOKEN_TO_INDEX[t] >= 5:\n",
    "\t\tbreak\n",
    "\tprint(f\"\\t'{t}':   \\t{VOCAB_TOKEN_TO_INDEX[t]}\")\n",
    "print(\"\\t...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations of Static Vocabulary\n",
    "\n",
    "- No more rasterized vs uniform indexing, it's all fixed as uniform now\n",
    "- Fixed max grid size\n",
    "  - There is now a fixed maximum maze size which is supported.\n",
    "  - Unique tokens (`CoordTokenizers.UT`): 50x50\n",
    "  - Coordinate tuple tokens (`CoordTokenizers.CTT`): 128x128\n",
    "  - Mazes larger than these sizes are not supported\n",
    "  - There should be fewer compatibility issues with tokenizers using different `max_grid_size` parameters\n",
    "- Vocabulary access\n",
    "  - Since maze-dataset 1.0, there is no need to pass around a tokenizer object or any data structure to access its custom vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring your code from legacy `MazeTokenizer` and `TokenizationMode`\n",
    "Since `MazeTokenizerModular` uses a static vocabulary, it is not backwards compatible with any models trained using a legacy `MazeTokenizer`. The `maze-transformer` library is updated in vX.X.X to use `MazeTokenizerModular` by default. \n",
    "\n",
    "If you've manually specified a `MazeTokenizer` or `TokenizationMode` in your research code, the easiest way to refactor is using `MazeTokenizerModular.from_legacy`, which will convert a `MazeTokenizer` or `TokenizationMode` to its corresponding `MazeTokenizerModular` instance. Note that this correspondence means only that the stringification of mazes are equivalent; the encodings of strings to integer vocabulary indices are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legacy_maze_tokenizer: MazeTokenizer = (\n",
    "\tTokenizationMode.AOTP_UT_uniform.to_legacy_tokenizer()\n",
    ")\n",
    "modular_tokenizer_equivalent: MazeTokenizerModular = MazeTokenizerModular.from_legacy(\n",
    "\tlegacy_maze_tokenizer,\n",
    ")\n",
    "print(legacy_maze_tokenizer, \"\\n\", modular_tokenizer_equivalent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_all_tokenizers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most combinations of `TokenizerElement`s and their arguments will produce a valid and unique `MazeTokenizerModular`. However, it is not guaranteed that every possible `MazeTokenizerModular` that can be constructed will make practical sense or have been put through testing.\n",
    "\n",
    "`get_all_tokenizers` constructs and caches all the tested tokenizers at once. For research investigating many different tokenization schemes, one practical way to access them is by looping through/sampling from `get_all_tokenizers()`. Be aware that the indexing of specific tokenizers may change without notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashes = get_all_tokenizer_hashes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(all_hashes)} or {shorten_numerical_to_str(len(all_hashes))} hashes found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokenizers = get_all_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "\tf\"{len(all_tokenizers)} or {shorten_numerical_to_str(len(all_tokenizers))} tokenizers found.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_hashes) == len(all_tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other possible tokenizers which aren't in `get_all_tokenizers` are not guaranteed to function. Instead of running the expensive call to `get_all_tokenizers` yourself, you can check if a tokenizer is tested using `MazeTokenizerModular.is_tested_tokenizer` or `MazeTokenizerModular.is_valid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mt_default.is_tested_tokenizer(do_assert=True)\n",
    "assert mt_default.is_valid()\n",
    "assert mt_ctt.is_tested_tokenizer()\n",
    "assert mt_ctt.is_valid()\n",
    "\n",
    "custom_untested_tokenizer = MazeTokenizerModular(\n",
    "\tprompt_sequencer=PromptSequencers.AOP(\n",
    "\t\tpath_tokenizer=PathTokenizers.StepSequence(\n",
    "\t\t\tstep_tokenizers=(StepTokenizers.Distance(),),\n",
    "\t\t),\n",
    "\t),\n",
    ")\n",
    "\n",
    "assert not custom_untested_tokenizer.is_tested_tokenizer()\n",
    "assert not custom_untested_tokenizer.is_valid()\n",
    "# Danger, use this tokenizer at your own risk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Tokenizer Collections\n",
    "\n",
    "There are a several practical ways to filter down a collection of tokenizers, or alternatively, generate a new collection with a filter.\n",
    "\n",
    "**WARNING: Applying `filter` to the output of `get_all_tokenizers` is extremely slow due to the size of the initial population. Only use the first 3 methods for filtering much smaller collections of tokenizers. To generate a new collection based on filters, always use `utils.all_instances`**\n",
    "\n",
    "In order of increasing speed, power and decreasing syntactic concision:\n",
    "\n",
    "1. `MazeTokenizerModular.has_element`\n",
    "    - Use case: Use with `filter` for concise, basic filtering on an existing collection\n",
    "1. `MazeTokenizerModular.tokenizer_elements`\n",
    "    - Use case: Use with `filter` for more precise filtering on an existing collection\n",
    "1. `MazeTokenizerModular.summary`\n",
    "    - Use case: Use with `filter` for more precise filtering on an existing collection\n",
    "1. `utils.all_instances`\n",
    "    - Use case: Generate a new collection with filter(s).\n",
    "    - Anytime you don't already have a small collection of tokenizers as the starting population.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_all = len(get_all_tokenizers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_1: list[MazeTokenizerModular] = list(\n",
    "\tall_instances(\n",
    "\t\tMazeTokenizerModular,\n",
    "\t\t{\n",
    "\t\t\t**MAZE_TOKENIZER_MODULAR_DEFAULT_VALIDATION_FUNCS,  # Always include this as the first item in the dict whenever calling `all_instances` with `MazeTokenizerModular` or any `_TokenizerElement`\n",
    "\t\t\tCoordTokenizers._CoordTokenizer: lambda x: isinstance(\n",
    "\t\t\t\tx,\n",
    "\t\t\t\tCoordTokenizers.UT,\n",
    "\t\t\t),\n",
    "\t\t\tStepTokenizers.StepTokenizerPermutation: lambda x: x[0]\n",
    "\t\t\t== StepTokenizers.Cardinal()\n",
    "\t\t\tand len(x) < 3,\n",
    "\t\t\tAdjListTokenizers._AdjListTokenizer: lambda x: isinstance(\n",
    "\t\t\t\tx,\n",
    "\t\t\t\tAdjListTokenizers.AdjListCardinal,\n",
    "\t\t\t),\n",
    "\t\t\tEdgeSubsets._EdgeSubset: lambda x: x\n",
    "\t\t\t== EdgeSubsets.ConnectionEdges(walls=False),\n",
    "\t\t},\n",
    "\t),\n",
    ")\n",
    "filtered_2: list[MazeTokenizerModular] = list(\n",
    "\tall_instances(\n",
    "\t\tMazeTokenizerModular,\n",
    "\t\t{\n",
    "\t\t\t**MAZE_TOKENIZER_MODULAR_DEFAULT_VALIDATION_FUNCS,  # Always include this as the first item in the dict whenever calling`all_instances` with `MazeTokenizerModular` or any `_TokenizerElement`\n",
    "\t\t\t_TokenizerElement: lambda x: x.is_valid()\n",
    "\t\t\tand not getattr(x, \"pre\", False)\n",
    "\t\t\tand not getattr(x, \"intra\", False)\n",
    "\t\t\tand not getattr(x, \"post\", False),  # Minimal delimiters everywhere...\n",
    "\t\t\tCoordTokenizers.CTT: lambda x: x.pre\n",
    "\t\t\tand x.intra\n",
    "\t\t\tand x.post,  # ...except for the coord tokens\n",
    "\t\t},\n",
    "\t),\n",
    ")\n",
    "filtered_3: list[MazeTokenizerModular] = list(\n",
    "\tall_instances(\n",
    "\t\tMazeTokenizerModular,\n",
    "\t\t{\n",
    "\t\t\t**MAZE_TOKENIZER_MODULAR_DEFAULT_VALIDATION_FUNCS,  # Always include this as the first item in the dict whenever calling `all_instances` with `MazeTokenizerModular` or any `_TokenizerElement`\n",
    "\t\t\tPromptSequencers._PromptSequencer: lambda x: isinstance(\n",
    "\t\t\t\tx,\n",
    "\t\t\t\tPromptSequencers.AOTP,\n",
    "\t\t\t),\n",
    "\t\t\tTargetTokenizers._TargetTokenizer: lambda x: x\n",
    "\t\t\t== TargetTokenizers.Unlabeled(),\n",
    "\t\t\tStepSizes.Singles: lambda x: False,  # noqa: ARG005\n",
    "\t\t},\n",
    "\t),\n",
    ")\n",
    "print(f\"filtered 1: {len(filtered_1)} tokenizers / {len_all} tokenizers\")\n",
    "print(f\"filtered 2: {len(filtered_2)} tokenizers / {len_all} tokenizers\")\n",
    "print(f\"filtered 3: {len(filtered_3)} tokenizers / {len_all} tokenizers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples below show equivalent methods of filtering one of the smaller collections above using options 1-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_has_element: list[MazeTokenizerModular] = list(\n",
    "\tfilter(lambda x: x.has_element(EdgePermuters.BothCoords()), filtered_1),\n",
    ")\n",
    "\n",
    "filtered_tokenizer_elements: list[MazeTokenizerModular] = list(\n",
    "\tfilter(lambda x: EdgePermuters.BothCoords() in x.tokenizer_elements, filtered_1),\n",
    ")\n",
    "\n",
    "filtered_summary: list[MazeTokenizerModular] = list(\n",
    "\tfilter(\n",
    "\t\tlambda x: x.summary()[\"edge_permuter\"] == EdgePermuters.BothCoords().name,\n",
    "\t\tfiltered_1,\n",
    "\t),\n",
    ")\n",
    "\n",
    "print(f\"filtered: {len(filtered_has_element)} tokenizers / {len_all} tokenizers\")\n",
    "\n",
    "assert set(filtered_has_element) == set(filtered_tokenizer_elements)\n",
    "print(f\"{set(filtered_has_element).symmetric_difference(set(filtered_summary)) = }\")\n",
    "assert set(filtered_has_element) == set(filtered_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TokenizerElement Behavior Reference\n",
    "\n",
    "For each primary `TokenizerElement`, tokenizations and encodings derived from the below maze are logged in DataFrames for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg: MazeDatasetConfig = MazeDatasetConfig(\n",
    "\tname=\"test\",\n",
    "\tgrid_n=3,\n",
    "\tn_mazes=1,\n",
    "\tmaze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    ")\n",
    "dataset: MazeDataset = MazeDataset.from_config(\n",
    "\tcfg,\n",
    "\tdo_download=False,\n",
    "\tload_local=False,\n",
    "\tdo_generate=True,\n",
    "\tsave_local=False,\n",
    "\tverbose=True,\n",
    "\tgen_parallel=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "mz: SolvedMaze = dataset[0]\n",
    "MazePlot(mz).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_elements_df(\n",
    "\telem_type: type[_TokenizerElement],\n",
    "\tencoding: bool = True,\n",
    "\t**to_tokens_kwargs,\n",
    ") -> pd.DataFrame:\n",
    "\tcolumns = [\"_TokenizerElement\", \"tokens\"]\n",
    "\tif encoding:\n",
    "\t\tcolumns.append(\"encoding\")\n",
    "\ttokenizers: pd.DataFrame = pd.DataFrame(columns=columns)\n",
    "\n",
    "\ttokenizers[\"_TokenizerElement\"] = list(\n",
    "\t\tall_instances(\n",
    "\t\t\telem_type,\n",
    "\t\t\tvalidation_funcs=MAZE_TOKENIZER_MODULAR_DEFAULT_VALIDATION_FUNCS,\n",
    "\t\t),\n",
    "\t)\n",
    "\ttokenizers[\"tokens\"] = tokenizers[\"_TokenizerElement\"].apply(\n",
    "\t\tlambda x: \" \".join(x.to_tokens(**to_tokens_kwargs)),\n",
    "\t)\n",
    "\tif encoding:\n",
    "\t\ttokenizers[\"encoding\"] = tokenizers[\"tokens\"].apply(\n",
    "\t\t\tlambda x: MazeTokenizerModular.encode(x),\n",
    "\t\t)\n",
    "\treturn tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CoordTokenizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_tokenizers = all_elements_df(\n",
    "\tCoordTokenizers._CoordTokenizer,\n",
    "\tcoord=mz.solution[0],\n",
    ")\n",
    "coord_tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency List Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjlist_tokenizers = all_elements_df(\n",
    "\tAdjListTokenizers._AdjListTokenizer,\n",
    "\tencoding=False,\n",
    "\tmaze=mz,\n",
    "\tcoord_tokenizer=CoordTokenizers.UT(),\n",
    ")\n",
    "adjlist_tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokenizers = all_elements_df(\n",
    "\tTargetTokenizers._TargetTokenizer,\n",
    "\ttargets=[mz.end_pos],\n",
    "\tcoord_tokenizer=CoordTokenizers.UT(),\n",
    ")\n",
    "target_tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tokenizers = all_elements_df(\n",
    "\tPathTokenizers._PathTokenizer,\n",
    "\tmaze=mz,\n",
    "\tcoord_tokenizer=CoordTokenizers.UT(),\n",
    ")\n",
    "path_tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Sequencers\n",
    "\n",
    "Currently, the only difference in possible prompt sequencers is the inclusion/exclusion of target tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_sequencers = [PromptSequencers.AOTP(), PromptSequencers.AOP()]\n",
    "columns = [\"_TokenizerElement\", \"tokens\"]\n",
    "tokenizers: pd.DataFrame = pd.DataFrame(columns=columns)\n",
    "\n",
    "tokenizers[\"_TokenizerElement\"] = prompt_sequencers\n",
    "tokenizers[\"tokens\"] = tokenizers[\"_TokenizerElement\"].apply(\n",
    "\tlambda x: \" \".join(x.to_tokens(maze=mz)),\n",
    ")\n",
    "tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample of `MazeTokenizerModular`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_size: int = 1_000\n",
    "\n",
    "tokenizers: list[MazeTokenizerModular] = random.sample(\n",
    "\tget_all_tokenizers(),\n",
    "\trandom_sample_size,\n",
    ")\n",
    "columns = [\"MazeTokenizerModular\", \"tokens\", \"encoding\", *mt_default.summary().keys()]\n",
    "df: pd.DataFrame = pd.DataFrame(columns=columns)\n",
    "\n",
    "df[\"MazeTokenizerModular\"] = tokenizers\n",
    "df[\"tokens\"] = df[\"MazeTokenizerModular\"].apply(\n",
    "\tlambda x: \" \".join(x.to_tokens(maze=mz)),\n",
    ")\n",
    "df.encoding = df.tokens.apply(MazeTokenizerModular.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in tqdm(\n",
    "\tmt_default.summary().keys(),\n",
    "\tdesc=\"Tokenizers\",\n",
    "\ttotal=len(mt_default.summary()),\n",
    "):\n",
    "\tdf[k] = df.apply(\n",
    "\t\tlambda x: x.MazeTokenizerModular.summary().get(k, None),  # noqa: B023\n",
    "\t\taxis=1,\n",
    "\t)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
