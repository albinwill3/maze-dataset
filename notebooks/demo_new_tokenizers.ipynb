{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze_dataset.tokenization import (\n",
    "    ALL_TOKENIZERS,\n",
    "    MazeTokenizer2,\n",
    "    TokenizerElement,\n",
    "    CoordTokenizers,\n",
    "    PromptSequencers,\n",
    ")\n",
    "\n",
    "from maze_dataset import (\n",
    "    VOCAB,\n",
    "    VOCAB_LIST,\n",
    "    VOCAB_TOKEN_TO_INDEX,\n",
    "    LatticeMazeGenerators,\n",
    "    MazeDataset,\n",
    "    MazeDatasetConfig,\n",
    ")\n",
    "\n",
    "from maze_dataset.tokenization.util import equal_except_adj_list_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `MazeTokenizer2` Initialization\n",
    "\n",
    "Most of the API for these tokenizers is contained in the `MazeTokenizer2` class. The only time when users need to interact with the internal components of a `MazeTokenizer2` is when initializing a non-default tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_default = MazeTokenizer2()\n",
    "mt_ctt = MazeTokenizer2(coord_tokenizer=CoordTokenizers.CTT())\n",
    "# mt_default, mt_ctt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 internal components of every `MazeTokenizer2`, each of which subclasses `TokenizerElement`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'maze_dataset.tokenization.maze_tokenizer.CoordTokenizers.CoordTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.AdjListTokenizers.AdjListTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.TargetTokenizers.TargetTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.PathTokenizers.PathTokenizer'>\n",
      "<class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers.PromptSequencer'>\n",
      "\n",
      "No-arg tokenizer defaults:\n",
      "PromptSequencers.AOTP()\n",
      "CoordTokenizers.UT()\n",
      "AdjListTokenizers.Coords(intra=True, post=True, walls=False)\n",
      "TargetTokenizers.Unlabeled(post=False)\n",
      "PathTokenizers.Coords(post=False)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([str(elem) for elem in TokenizerElement.__subclasses__()]))\n",
    "print('\\nNo-arg tokenizer defaults:')\n",
    "print(\"\\n\".join([str(elem) for elem in mt_default._tokenizer_elements]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some `TokenizerElement`s also contain their own initialization arguments. Currently, all of these arguments are `boolean`-typed, but more complex args could be added in the future. The most common arguments across all `TokenizerElement`s are `pre`, `intra`, and `post`, which all control the option to add delimiter tokens to that part of the output. Other args are more specialized; see the docstrings for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "\n",
    "All instances of `MazeTokenizer2` uses a static vocabulary `VOCAB`, which is one of the main functional differences from `MazeTokenizer`. Direct access to the static vocabulary can be made through 3 constants:\n",
    "- `VOCAB`: extension of the existing `SPECIAL_TOKENS` dataclass\n",
    "- `VOCAB_LIST: list[str]`\n",
    "- `VOCAB_TOKEN_TO_INDEX: dict[str, int]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`VOCAB`: type\n",
      "\tADJLIST_START\n",
      "\tADJLIST_END\n",
      "\tTARGET_START\n",
      "\tTARGET_END\n",
      "\tORIGIN_START\n",
      "\n",
      "`VOCAB_LIST`: list[str]\n",
      "\t<ADJLIST_START>\n",
      "\t<ADJLIST_END>\n",
      "\t<TARGET_START>\n",
      "\t<TARGET_END>\n",
      "\t<ORIGIN_START>\n",
      "\n",
      "`VOCAB_TOKEN_TO_INDEX`: dict[str, int]\n",
      "\t<ADJLIST_START>:   \t0\n",
      "\t<ADJLIST_END>:   \t1\n",
      "\t<TARGET_START>:   \t2\n",
      "\t<TARGET_END>:   \t3\n",
      "\t<ORIGIN_START>:   \t4\n"
     ]
    }
   ],
   "source": [
    "print(\"`VOCAB`: type\")\n",
    "for i, t in enumerate(VOCAB):\n",
    "    if i >= 5: break\n",
    "    print(f\"\\t{t}\")\n",
    "\n",
    "print(\"\\n`VOCAB_LIST`: list[str]\")\n",
    "for t in VOCAB_LIST[:5]:\n",
    "    print(f\"\\t{t}\")\n",
    "    \n",
    "print(\"\\n`VOCAB_TOKEN_TO_INDEX`: dict[str, int]\")\n",
    "for t in VOCAB_TOKEN_TO_INDEX:\n",
    "    if VOCAB_TOKEN_TO_INDEX[t] >= 5: break\n",
    "    print(f\"\\t{t}:   \\t{VOCAB_TOKEN_TO_INDEX[t]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations of Static Vocabulary\n",
    "\n",
    "- No more rasterized vs uniform indexing\n",
    "- Fixed max grid size\n",
    "  - There is now a fixed maximum maze size which is supported.\n",
    "  - Unique tokens (`CoordTokenizers.UT`): 50x50\n",
    "  - Coordinate tuple tokens (`CoordTokenizers.CTT`): 128x128\n",
    "  - Mazes larger than these sizes are not supported\n",
    "  - There should be fewer compatibility issues with tokenizers using different `max_grid_size` parameters\n",
    "- Vocabulary access\n",
    "  - There is now no need to pass around a tokenizer object or any parameter to access its custom vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring your code from legacy `MazeTokenizer` and `TokenizationMode`\n",
    "Since `MazeTokenizer2` uses a static vocabulary, it is not backwards compatible with any models trained using a legacy `MazeTokenizer`. The Unsearch libraries will soon be updated to use `MazeTokenizer2` by default in all settings. \n",
    "\n",
    "If you've manually specified a `MazeTokenizer` or `TokenizationMode` in your research code, the easiest way to refactor is using `MazeTokenizer2.from_legacy`, which will convert a `MazeTokenizer` or `TokenizationMode` to its corresponding `MazeTokenizer2` instance. Note that this correspondence means only that the tokenization of mazes to strings are equivalent; the encodings to integer vocabulary indices are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ALL_TOKENIZERS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, all combinations of `TokenizerElement`s and their arguments will produce a valid and unique `MazeTokenizer2`. However, it is not guaranteed that every possible `MazeTokenizer2` that can be constructed will make practical sense or have been put through testing. `ALL_TOKENIZERS` contains all the tested tokenizers pre-built. For research investigating many different tokenization schemes, one practical way to access them is by looping through `ALL_TOKENIZERS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to get the dataset 'test-g5-n1-a_dfs-h919'\n",
      "generating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating & solving mazes: 100%|██████████| 1/1 [00:00<00:00, 66.71maze/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got dataset test with 1 items. output.cfg.to_fname() = 'test-g5-n1-a_dfs-h919'\n",
      "\n",
      "\n",
      "MazeTokenizer2-PromptSequencers.AOTP()-CoordTokenizers.UT()-AdjListTokenizers.Coords(intra=True, post=True, walls=False)-TargetTokenizers.Unlabeled(post=False)-PathTokenizers.Coords(post=False)\n",
      "['<ADJLIST_START>', '(0,0)', '<-->', '(1,0)', ';', '(2,0)', '<-->', '(3,0)', ';', '(4,1)', '<-->', '(4,0)', ';', '(2,0)', '<-->', '(2,1)', ';', '(1,0)', '<-->', '(1,1)', ';', '(3,4)', '<-->', '(2,4)', ';', '(4,2)', '<-->', '(4,3)', ';', '(0,0)', '<-->', '(0,1)', ';', '(0,3)', '<-->', '(0,2)', ';', '(4,4)', '<-->', '(3,4)', ';', '(4,3)', '<-->', '(4,4)', ';', '(4,1)', '<-->', '(4,2)', ';', '(2,1)', '<-->', '(2,2)', ';', '(1,4)', '<-->', '(0,4)', ';', '(1,2)', '<-->', '(0,2)', ';', '(2,4)', '<-->', '(2,3)', ';', '(4,0)', '<-->', '(3,0)', ';', '(2,2)', '<-->', '(3,2)', ';', '(1,2)', '<-->', '(2,2)', ';', '(1,3)', '<-->', '(0,3)', ';', '(3,2)', '<-->', '(3,3)', ';', '(0,2)', '<-->', '(0,1)', ';', '(3,1)', '<-->', '(3,2)', ';', '(1,3)', '<-->', '(1,4)', ';', '<ADJLIST_END>', '<ORIGIN_START>', '(1,3)', '<ORIGIN_END>', '<TARGET_START>', '(2,3)', '<TARGET_END>', '<PATH_START>', '(1,3)', '(0,3)', '(0,2)', '(1,2)', '(2,2)', '(2,1)', '(2,0)', '(3,0)', '(4,0)', '(4,1)', '(4,2)', '(4,3)', '(4,4)', '(3,4)', '(2,4)', '(2,3)', '<PATH_END>']\n",
      "['<ADJLIST_START>', '(3,1)', '<-->', '(3,2)', ';', '(1,4)', '<-->', '(0,4)', ';', '(3,0)', '<-->', '(2,0)', ';', '(0,2)', '<-->', '(1,2)', ';', '(1,2)', '<-->', '(2,2)', ';', '(3,4)', '<-->', '(4,4)', ';', '(4,0)', '<-->', '(4,1)', ';', '(2,3)', '<-->', '(2,4)', ';', '(2,1)', '<-->', '(2,0)', ';', '(1,1)', '<-->', '(1,0)', ';', '(2,1)', '<-->', '(2,2)', ';', '(0,3)', '<-->', '(1,3)', ';', '(0,1)', '<-->', '(0,0)', ';', '(4,1)', '<-->', '(4,2)', ';', '(3,4)', '<-->', '(2,4)', ';', '(0,2)', '<-->', '(0,1)', ';', '(3,2)', '<-->', '(3,3)', ';', '(4,3)', '<-->', '(4,2)', ';', '(2,2)', '<-->', '(3,2)', ';', '(0,0)', '<-->', '(1,0)', ';', '(4,3)', '<-->', '(4,4)', ';', '(1,4)', '<-->', '(1,3)', ';', '(4,0)', '<-->', '(3,0)', ';', '(0,3)', '<-->', '(0,2)', ';', '<ADJLIST_END>', '<ORIGIN_START>', '(1,3)', '<ORIGIN_END>', '<TARGET_START>', '(2,3)', '<TARGET_END>', '<PATH_START>', '(1,3)', '(0,3)', '(0,2)', '(1,2)', '(2,2)', '(2,1)', '(2,0)', '(3,0)', '(4,0)', '(4,1)', '(4,2)', '(4,3)', '(4,4)', '(3,4)', '(2,4)', '(2,3)', '<PATH_END>']\n",
      "\n",
      "MazeTokenizer2-PromptSequencers.AOTP()-CoordTokenizers.CTT(pre=True, intra=True, post=True)-AdjListTokenizers.Coords(intra=True, post=True, walls=False)-TargetTokenizers.Unlabeled(post=False)-PathTokenizers.Coords(post=False)\n",
      "['<ADJLIST_START>', '(', '0', ',', '0', ')', '<-->', '(', '1', ',', '0', ')', ';', '(', '2', ',', '1', ')', '<-->', '(', '2', ',', '2', ')', ';', '(', '1', ',', '1', ')', '<-->', '(', '1', ',', '0', ')', ';', '(', '3', ',', '4', ')', '<-->', '(', '2', ',', '4', ')', ';', '(', '3', ',', '2', ')', '<-->', '(', '3', ',', '3', ')', ';', '(', '2', ',', '4', ')', '<-->', '(', '2', ',', '3', ')', ';', '(', '0', ',', '1', ')', '<-->', '(', '0', ',', '2', ')', ';', '(', '4', ',', '3', ')', '<-->', '(', '4', ',', '2', ')', ';', '(', '4', ',', '0', ')', '<-->', '(', '3', ',', '0', ')', ';', '(', '4', ',', '4', ')', '<-->', '(', '3', ',', '4', ')', ';', '(', '1', ',', '2', ')', '<-->', '(', '0', ',', '2', ')', ';', '(', '2', ',', '0', ')', '<-->', '(', '3', ',', '0', ')', ';', '(', '1', ',', '3', ')', '<-->', '(', '0', ',', '3', ')', ';', '(', '3', ',', '1', ')', '<-->', '(', '3', ',', '2', ')', ';', '(', '4', ',', '1', ')', '<-->', '(', '4', ',', '2', ')', ';', '(', '2', ',', '1', ')', '<-->', '(', '2', ',', '0', ')', ';', '(', '4', ',', '0', ')', '<-->', '(', '4', ',', '1', ')', ';', '(', '4', ',', '3', ')', '<-->', '(', '4', ',', '4', ')', ';', '(', '0', ',', '0', ')', '<-->', '(', '0', ',', '1', ')', ';', '(', '1', ',', '4', ')', '<-->', '(', '1', ',', '3', ')', ';', '(', '0', ',', '3', ')', '<-->', '(', '0', ',', '2', ')', ';', '(', '2', ',', '2', ')', '<-->', '(', '3', ',', '2', ')', ';', '(', '1', ',', '2', ')', '<-->', '(', '2', ',', '2', ')', ';', '(', '0', ',', '4', ')', '<-->', '(', '1', ',', '4', ')', ';', '<ADJLIST_END>', '<ORIGIN_START>', '(', '1', ',', '3', ')', '<ORIGIN_END>', '<TARGET_START>', '(', '2', ',', '3', ')', '<TARGET_END>', '<PATH_START>', '(', '1', ',', '3', ')', '(', '0', ',', '3', ')', '(', '0', ',', '2', ')', '(', '1', ',', '2', ')', '(', '2', ',', '2', ')', '(', '2', ',', '1', ')', '(', '2', ',', '0', ')', '(', '3', ',', '0', ')', '(', '4', ',', '0', ')', '(', '4', ',', '1', ')', '(', '4', ',', '2', ')', '(', '4', ',', '3', ')', '(', '4', ',', '4', ')', '(', '3', ',', '4', ')', '(', '2', ',', '4', ')', '(', '2', ',', '3', ')', '<PATH_END>']\n",
      "['<ADJLIST_START>', '(', '0', ',', '1', ')', '<-->', '(', '0', ',', '2', ')', ';', '(', '1', ',', '0', ')', '<-->', '(', '1', ',', '1', ')', ';', '(', '0', ',', '1', ')', '<-->', '(', '0', ',', '0', ')', ';', '(', '4', ',', '0', ')', '<-->', '(', '3', ',', '0', ')', ';', '(', '0', ',', '2', ')', '<-->', '(', '0', ',', '3', ')', ';', '(', '0', ',', '4', ')', '<-->', '(', '1', ',', '4', ')', ';', '(', '2', ',', '2', ')', '<-->', '(', '3', ',', '2', ')', ';', '(', '3', ',', '4', ')', '<-->', '(', '2', ',', '4', ')', ';', '(', '2', ',', '0', ')', '<-->', '(', '3', ',', '0', ')', ';', '(', '3', ',', '4', ')', '<-->', '(', '4', ',', '4', ')', ';', '(', '3', ',', '2', ')', '<-->', '(', '3', ',', '3', ')', ';', '(', '2', ',', '4', ')', '<-->', '(', '2', ',', '3', ')', ';', '(', '1', ',', '3', ')', '<-->', '(', '0', ',', '3', ')', ';', '(', '1', ',', '3', ')', '<-->', '(', '1', ',', '4', ')', ';', '(', '1', ',', '0', ')', '<-->', '(', '0', ',', '0', ')', ';', '(', '2', ',', '1', ')', '<-->', '(', '2', ',', '0', ')', ';', '(', '4', ',', '2', ')', '<-->', '(', '4', ',', '3', ')', ';', '(', '3', ',', '2', ')', '<-->', '(', '3', ',', '1', ')', ';', '(', '1', ',', '2', ')', '<-->', '(', '0', ',', '2', ')', ';', '(', '4', ',', '1', ')', '<-->', '(', '4', ',', '0', ')', ';', '(', '4', ',', '2', ')', '<-->', '(', '4', ',', '1', ')', ';', '(', '4', ',', '3', ')', '<-->', '(', '4', ',', '4', ')', ';', '(', '1', ',', '2', ')', '<-->', '(', '2', ',', '2', ')', ';', '(', '2', ',', '1', ')', '<-->', '(', '2', ',', '2', ')', ';', '<ADJLIST_END>', '<ORIGIN_START>', '(', '1', ',', '3', ')', '<ORIGIN_END>', '<TARGET_START>', '(', '2', ',', '3', ')', '<TARGET_END>', '<PATH_START>', '(', '1', ',', '3', ')', '(', '0', ',', '3', ')', '(', '0', ',', '2', ')', '(', '1', ',', '2', ')', '(', '2', ',', '2', ')', '(', '2', ',', '1', ')', '(', '2', ',', '0', ')', '(', '3', ',', '0', ')', '(', '4', ',', '0', ')', '(', '4', ',', '1', ')', '(', '4', ',', '2', ')', '(', '4', ',', '3', ')', '(', '4', ',', '4', ')', '(', '3', ',', '4', ')', '(', '2', ',', '4', ')', '(', '2', ',', '3', ')', '<PATH_END>']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cfg: MazeDatasetConfig = MazeDatasetConfig(\n",
    "    name=\"test\",\n",
    "    grid_n=5,\n",
    "    n_mazes=1,\n",
    "    maze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    ")\n",
    "maze_dataset: MazeDataset = MazeDataset.from_config(\n",
    "    cfg,\n",
    "    do_download=False,\n",
    "    load_local=False,\n",
    "    do_generate=True,\n",
    "    save_local=False,\n",
    "    verbose=True,\n",
    "    gen_parallel=False,\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "for tokenizer in ALL_TOKENIZERS:\n",
    "    print(tokenizer)\n",
    "    a: list[str] = maze_dataset[0].as_tokens(tokenizer)\n",
    "    b: list[str] = tokenizer.to_tokens(maze_dataset[0])\n",
    "    assert equal_except_adj_list_sequence(a, b)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other possible tokenizers which aren't in `ALL_TOKENIZERS` are not guaranteed to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aaron\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-dataset-zQQMZP3O-py3.10\\lib\\site-packages\\muutils\\json_serialize\\serializable_dataclass.py:171: UserWarning: Cannot compare PromptSequencers.AOTP() and PromptSequencers.AOP(include_target_special_tokens=False) for equality due to classes not matching: <class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers.AOTP'> vs <class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers.AOP'>\n",
      "  warnings.warn(\n",
      "c:\\Users\\Aaron\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-dataset-zQQMZP3O-py3.10\\lib\\site-packages\\muutils\\json_serialize\\serializable_dataclass.py:171: UserWarning: Cannot compare PromptSequencers.AOP(include_target_special_tokens=False) and PromptSequencers.AOTP() for equality due to classes not matching: <class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers.AOP'> vs <class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers.AOTP'>\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "custom_tokenizer = MazeTokenizer2(\n",
    "    prompt_sequencer=PromptSequencers.AOP(include_target_special_tokens=False),\n",
    ")\n",
    "\n",
    "assert custom_tokenizer not in ALL_TOKENIZERS  # Danger, use at your own risk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-dataset-zQQMZP3O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
